{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Usecase\n",
    "\n",
    "We all copied this code at least once in our life:\n",
    "\n",
    "```python\n",
    ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.33, random_state=42)\n",
    "```\n",
    "\n",
    "and even if it's not that long, it's still a bit annoying to write/ copy. It must be a better way to do this. And there is! With the `Dataset` class, you can do this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skdataset.dataset import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=10)\n",
    "\n",
    "# make some data\n",
    "X, y = make_classification(\n",
    "    n_samples=200, \n",
    "    n_features=4, \n",
    "    n_classes=2, \n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "ds = Dataset(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = train_test_split(ds, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds) + len(test_ds) == len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This by itself if not that impressive, but it's a good start. Let's see what else we can do with this class.\n",
    "We also can manipulate the data in the dataset with `transform` method without changing the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.63315862, -0.74435214,  1.13876335, -0.46847537],\n",
       "       [-0.92523065, -0.29979364,  7.23459721, -5.21427765],\n",
       "       [ 2.60788321,  4.11773656,  2.7461706 , -4.11749094]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiply_by_2(dataset: Dataset):\n",
    "    dataset.X = dataset.X * 2\n",
    "    return dataset\n",
    "\n",
    "ds.transform(multiply_by_2)[:3, 'X'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31657931, -0.37217607,  0.56938167, -0.23423769],\n",
       "       [-0.46261533, -0.14989682,  3.61729861, -2.60713883],\n",
       "       [ 1.3039416 ,  2.05886828,  1.3730853 , -2.05874547]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:3, 'X']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or even filter some rows based on a condition and that filter will be applied to all the variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[-3.16579308e-01, -3.72176068e-01,  5.69381673e-01,\n",
       "         -2.34237685e-01],\n",
       "        [ 1.30394160e+00,  2.05886828e+00,  1.37308530e+00,\n",
       "         -2.05874547e+00],\n",
       "        [ 2.34871791e+00,  3.66332221e+00,  2.15367981e+00,\n",
       "         -3.44843438e+00],\n",
       "        ...,\n",
       "        [ 4.57034850e-01,  7.10998715e-01,  4.06047306e-01,\n",
       "         -6.60427991e-01],\n",
       "        [ 2.95498748e-03,  2.41383787e-01,  1.67668118e+00,\n",
       "         -1.36553500e+00],\n",
       "        [ 5.82578896e-01,  8.75518424e-01,  2.99929732e-01,\n",
       "         -6.64855018e-01]]),\n",
       " 'y': array([1, 1, 1, ..., 1, 1, 1])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.filter(lambda x: x.y == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abvious benefit of this is that you always have all of your data together, so you dont have to filter them once, and map the filter to the other parts(e.g. filtering some rows on X and then apply the same filter on y, or sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will interduce `DatasetDict` which is a dictionary-like object that holds multiple `Dataset` objects. It's very useful when you have multiple datasets(e.g. train, val and test) that you want to keep together.\n",
    "\n",
    "You can create one by jast passing a dict with some keys and `Dataset` objects as values or call `split` with a spliter function on your dataset. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skdataset import DatasetDict\n",
    "\n",
    "ds_dict = DatasetDict({'train': train_ds, 'test': test_ds})\n",
    "\n",
    "ds_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds_dict = ds.split(train_test_split, test_size=0.33, random_state=42)\n",
    "\n",
    "ds_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new object has some handy attributes like `X_train` or `y_test` which all automatically generated from the keys in the dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.08384298,  1.63367133,  0.59219328, -1.26471893],\n",
       "       [ 2.0629089 ,  3.58425588,  4.48422581, -5.13700395],\n",
       "       [ 1.39912667,  2.25751204,  1.81515089, -2.48699264],\n",
       "       ...,\n",
       "       [ 0.52175103,  0.68583542, -0.4261365 , -0.03049782],\n",
       "       [ 0.24602058,  0.69302302,  2.41232056, -2.1393599 ],\n",
       "       [ 1.64450901,  2.64268527,  2.05746083, -2.86133886]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict.X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is equal to saying `ds_dict['train']['X']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a function that you want to apply to all of your splits now, instead of looping over them, you can do `ds_dict.transform(func)` and it will apply the function to all of the splits and return a new `DatasetDict` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'X': array([[  2.16768595,   3.26734266,   1.18438656,  -2.52943786],\n",
       "         [  4.1258178 ,   7.16851176,   8.96845162, -10.27400789],\n",
       "         [  2.79825335,   4.51502408,   3.63030177,  -4.97398527],\n",
       "         ...,\n",
       "         [  1.04350205,   1.37167084,  -0.852273  ,  -0.06099565],\n",
       "         [  0.49204117,   1.38604604,   4.82464112,  -4.27871979],\n",
       "         [  3.28901801,   5.28537054,   4.11492167,  -5.72267771]]),\n",
       "  'y': array([1, 1, 1, ..., 0, 0, 1])},\n",
       " 'test': {'X': array([[ 2.6358973 ,  4.3559125 ,  4.14682446, -5.27668074],\n",
       "         [ 1.75768303,  3.10428922,  4.17675655, -4.66643081],\n",
       "         [-2.29434589, -3.29988447, -0.13391834,  1.76676886],\n",
       "         ...,\n",
       "         [-0.0109273 ,  0.97612538,  7.01157169, -5.69359055],\n",
       "         [-1.82972602, -2.19872701,  2.9538225 , -1.07976942],\n",
       "         [ 2.58931623,  3.67742897, -0.17904181, -1.72542688]]),\n",
       "  'y': array([1, 1, 0, ..., 0, 0, 0])}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict.transform(multiply_by_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use all of these together. If we wanted to do it without `Dataset` It would be something like this:\n",
    "\n",
    "```python\n",
    "from skdataset.dataset import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=200, \n",
    "    n_features=4, \n",
    "    n_classes=2, \n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print('Train score:', model.score(X_train, y_train))\n",
    "print('Test score:', model.score(X_test, y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.0\n",
      "Test score: 0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "from skdataset.dataset import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=200, \n",
    "    n_features=4, \n",
    "    n_classes=2, \n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "ds_dict = Dataset(X=X, y=y).split(train_test_split, test_size=0.33, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(**ds_dict['train'])\n",
    "\n",
    "print('Train score:', model.score(**ds_dict['train']))\n",
    "print('Test score:', model.score(**ds_dict['test']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
